# AI Agent Strategy for DataBridge AI

This document outlines the current AI architecture of the DataBridge platform and proposes an evolution towards a true multi-agent system to enhance its capabilities, particularly for the automated data warehouse creation workflow.

---

## 1. Current AI Architecture: Single Orchestrator with "Skills"

The current design of DataBridge AI is centered around a **single, primary AI orchestrator** (e.g., Gemini, Claude) that directs the application's tools. This orchestrator is not a monolithic entity; it can adopt different "Skills" based on the task at hand.

These "Skills" are not independent agents but rather **expert personas** defined by specialized system prompts. They guide the primary AI's reasoning, communication style, and tool usage.

### Existing "Agents" (Personas/Skills)

The existing plans for V3 and V4 already define a powerful set of skills:

*   **`Data Analyst`:** Focuses on SQL queries and ad-hoc data exploration.
*   **`BI Developer`:** Specializes in schema design, data modeling, and optimization.
*   **`Data Scientist`:** Performs advanced statistical analysis and identifies patterns.
*   **`Business User` / `NL Query Assistant`:** Translates plain English questions into actionable queries, hiding technical complexity.
*   **`Data Steward` / `Variance Analyst`:** Focuses on data quality, governance, and detailed analysis like Budget vs. Actuals.
*   **`SQL Expert`:** excels at writing and debugging complex SQL.
*   **`Executive Consumer` / `Management Reporter`:** Summarizes data into high-level insights, KPIs, and reports.

While effective, this model still relies on a single AI to manage the entire workflow. To truly automate the complex task of data warehouse creation, we can introduce a team of specialized, collaborating agents.

---

## 2. Proposed New Architecture: A Multi-Agent System

I propose creating a new layer in the DataBridge application for a team of specialized AI agents. The primary AI orchestrator would act as a "Project Manager," delegating specific, well-defined tasks to these expert agents.

### Proposed New AI Agents

#### 1. The `Schema Scanner` Agent
*   **Purpose:** To be the expert on technical metadata. Its sole function is to connect to source systems and produce a structured, technical inventory of the data.
*   **Inputs:** A connection string or file path.
*   **Tools:** It would exclusively use the `databridge-core` connector tools to read schemas, tables, columns, data types, and explicit foreign key relationships.
*   **Output:** A clean JSON or YAML object representing the technical schema, free of any business interpretation.

#### 2. The `Logic Extractor` Agent
*   **Purpose:** To be the expert in reverse-engineering business logic from legacy assets.
*   **Inputs:** Complex SQL query files (like `FP&A Queries.sql`).
*   **Tools:** It would use the `sqlglot` parser to create an Abstract Syntax Tree (AST), and graph analysis tools (`networkx`) to traverse the AST. Its core intelligence would be in identifying patterns, especially massive `CASE` statements, and translating them into a structured hierarchy format.
*   **Output:** A JSON object defining the discovered hierarchies (e.g., parent-child relationships for a Chart of Accounts).

#### 3. The `Warehouse Architect` Agent
*   **Purpose:** To be the master data modeler. This agent embodies the best practices of data warehouse design.
*   **Inputs:** The technical schema from the `Schema Scanner` and the extracted business logic from the `Logic Extractor`.
*   **Tools:** It uses its internal knowledge (a specialized system prompt) of star schemas, snowflake schemas, and the specific DataBridge object model (`TBL_0`, `DT_2`, `DT_3A`, etc.) to design the optimal data warehouse. It synthesizes the inputs into a coherent, deployable plan.
*   **Output:** A complete dbt project, including all necessary `.sql` and `.yml` files, ready for deployment.

#### 4. The `Deploy & Validate` Agent
*   **Purpose:** To be the DevOps and QA specialist. It is a purely technical agent focused on execution and verification.
*   **Inputs:** A dbt project generated by the `Warehouse Architect` and a target environment (e.g., "staging").
*   **Tools:** It uses Git tools to commit the project to a repository, and it uses dbt Core or dbt Cloud APIs to run `dbt build` and `dbt test`. It then uses V4's analytics tools to run post-deployment validation queries (e.g., row count comparisons).
*   **Output:** A success or failure report, including any test failures or data validation errors.

### How the New Agents Would Interact: A Sample Workflow

This multi-agent system transforms a complex user request into a managed, autonomous workflow.

1.  **User Prompt:**
    > "I need a new data mart for my sales analysis. Here is the connection to our production database and a folder of my team's old analysis queries. Please build it for me and save it to a new GitHub repo called `sales-datamart`."

2.  **Primary Orchestrator (Gemini) takes over as Project Manager:**
    > "Understood. I will dispatch my team to build your sales data mart."

3.  **Delegation (Parallel Execution):**
    *   The orchestrator invokes the **`Schema Scanner` Agent** with the database connection string.
    *   Simultaneously, it invokes the **`Logic Extractor` Agent** with the folder of SQL queries.

4.  **Synthesis:**
    *   The `Schema Scanner` returns a JSON file of the raw database schema.
    *   The `Logic Extractor` returns a JSON file defining three potential hierarchies it found: 'Product Categories', 'Sales Regions', and 'Deal Size'.
    *   The orchestrator reviews these outputs. It might ask the user for clarification: "I found three hierarchies. Do you want to include all of them in this data mart?"

5.  **Design:**
    *   Once the user confirms, the orchestrator invokes the **`Warehouse Architect` Agent**: "Please design a sales data mart using this technical schema and these three hierarchies. Generate a dbt project."
    *   The `Warehouse Architect` agent designs the star schema, creates the dbt model files, and returns the path to the finished dbt project.

6.  **Execution:**
    *   The orchestrator invokes the **`Deploy & Validate` Agent**: "Please deploy this dbt project to the `staging` environment. Commit it to a new GitHub repo named `sales-datamart` first, then run all tests."
    *   The `Deploy & Validate` agent performs the technical tasks and returns a final report.

7.  **Completion:**
    > "I'm pleased to report that your new `sales-datamart` has been successfully created, tested, and deployed to the staging environment. The dbt project is available at `github.com/your-org/sales-datamart`. You can now start asking questions about your sales data."

This multi-agent approach provides a clear separation of concerns, allows for parallel execution of tasks, and makes the entire system more robust, scalable, and easier to maintain.
# Plan: The DataBridge Historian - A Context and Memory System

This document proposes the creation of a new core system within the DataBridge AI platform called **The Historian**. Its purpose is to provide a persistent, queryable, and intelligent "memory" of all significant events and interactions, enabling true session-to-session context for both individual users and entire organizations.

---

## 1. The Problem: Context Amnesia

Currently, the DataBridge AI's memory is limited. It retains the final data assets (hierarchies, mappings) but forgets the conversational and operational history once a session ends. This means:

*   Users must re-establish context every time they start a new session.
*   The AI cannot learn from past interactions to improve its future performance.
*   It's difficult to audit the sequence of events that led to a particular data model or hierarchy.

The Historian is designed to solve this by creating an ongoing, intelligent log of all platform activities.

---

## 2. Proposed Solution: The Historian Architecture

The Historian will be a new, independent library (`libs/databridge-historian`) responsible for logging, storing, and retrieving context.

### 2.1. The Event Log Data Model

Every significant action taken by a user or the AI will be recorded as a structured **Event**. Each event will contain the following fields:

*   **`event_id`**: A unique identifier for the event.
*   **`session_id`**: Groups all events that occur within a single user session.
*   **`timestamp`**: The precise UTC time the event occurred.
*   **`user_id`**: The identifier for the user who initiated the action.
*   **`organization_id`**: The identifier for the company, allowing for both private (user) and shared (organization) context.
*   **`event_type`**: The category of the event. Examples include:
    *   `USER_PROMPT`: The user's direct command or question.
    *   `AI_TOOL_CALL`: The AI decides to use a tool.
    *   `TOOL_RESULT`: The output of that tool call.
    *   `AI_REASONING`: The AI's internal "thought process" on why it's taking an action. This is crucial for future understanding.
    *   `AI_ANALYSIS`: A summary or analysis generated by the AI.
    *   `USER_FEEDBACK`: A user's confirmation, cancellation, or correction of an AI action.
    *   `DATA_ASSET_CHANGE`: A record of a change to a persistent asset (e.g., a hierarchy was created, a mapping was updated).
*   **`payload`**: A flexible JSON object containing the specific data for the event (e.g., the tool name and arguments for `AI_TOOL_CALL`, the textual content for `AI_ANALYSIS`).

### 2.2. Storage Backend

To support this rich data model, a dual-database approach is recommended:

1.  **Primary Storage (Document Database):**
    *   **Technology:** MongoDB or PostgreSQL (using its JSONB capabilities).
    *   **Purpose:** To store the structured event logs. This allows for fast, indexed querying on fields like `user_id`, `organization_id`, `event_type`, and `timestamp`. This is the "factual" memory of what happened, when, and by whom.

2.  **Semantic Storage (Vector Database):**
    *   **Technology:** The existing vector database used by the V4 analytics engine.
    *   **Purpose:** To store embeddings of the textual content from the `payload` and `ai_reasoning` fields. This enables powerful semantic search over the history, allowing the AI (or users) to ask questions like, "What were we discussing last week about revenue recognition?" or "Find all past sessions where we mapped source data from the 'Enertia' system."

---

## 3. How the AI and Users Would Leverage the Historian

The Historian transforms the user experience by giving the AI a long-term memory.

### Example Workflow:

1.  **Session Start:**
    *   **User:** `gemini connect`
    *   **AI (in the background):** "This is `user-123` from `organization-abc`. I will now retrieve the summary of their last five sessions and the latest company-wide data asset changes." The AI queries the Historian to get up to speed.

2.  **Resuming Work:**
    *   **User:** "Okay, let's pick up where we left off."
    *   **AI:** "Certainly. In our last session, we were in the process of analyzing discrepancies in the `gl_line_item` mapping. We identified 11 mismatches. Would you like me to display that report again, or shall we proceed with creating a plan to resolve them?" (The AI knows this by querying for the last `event_type` for that `user_id`).

3.  **Company-Wide Knowledge:**
    *   **User:** "Has anyone else in my organization mapped a 'Chart of Accounts' from a JD Edwards system before?"
    *   **AI:** "Yes. On January 15th, `user-456` completed a project that involved mapping a JDE-based COA. I can show you the documentation they generated. This may help accelerate your current task. Would you like to see it?" (The AI performs a semantic search on the vector store of past events for `organization-abc`).

### Proposed New MCP Tools

To interact with this system, a new set of tools would be required:

*   **`historian_log_event(event_type: str, payload: json)`**: Used internally by the AI to record new events as they happen.
*   **`historian_get_my_history(limit: int = 5) -> json`**: Retrieves the most recent event history for the current user.
*   **`historian_summarize_session(session_id: str) -> str`**: Uses an LLM to provide a concise summary of a specific past session.
*   **`historian_search_org_history(query: str) -> json`**: Performs a semantic search across the entire organization's event history.

By implementing the Historian, DataBridge AI will gain a powerful, persistent memory, making it a more intuitive, efficient, and truly "intelligent" system.
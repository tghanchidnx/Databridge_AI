# Data Viewer & Workflow Visualization Architecture

## 1. Vision & Goals

This document outlines the architecture for two new, interconnected features for the DataBridge AI platform:

1.  **Data Viewer:** A component that exposes intermediate and final data from any tool in any process. This enables real-time data access for BI tools and other external systems, making the "headless" operations of DataBridge transparent.
2.  **Workflow Visualization:** A dynamic data flow diagram that visually represents the sequence of tools executed in a user's workflow.

The primary goal is to provide deep visibility into the inner workings of DataBridge AI without coupling it to a specific UI, reinforcing its "UI-agnostic" and "headless" philosophy.

## 2. Architecture for the "Data Viewer"

The Data Viewer will be implemented as a new set of REST API endpoints hosted within the existing **`databridge-researcher`** application. The Researcher's NestJS backend is the ideal location for this, as it is already set up to be a robust, extensible API server.

### 2.1. Data Snapshot Storage

When a tool executes, its output data will be stored as a "snapshot."

*   **Storage Mechanism:** For the initial implementation, we will use **Redis** as the storage backend for snapshots.
    *   **Why Redis?** It is extremely fast, already part of the project's stack, and its "time-to-live" (TTL) feature is perfect for automatically expiring and cleaning up temporary snapshot data.
*   **Data Format:** DataFrames will be serialized to **Apache Parquet** format before being stored in Redis.
    *   **Why Parquet?** It is a highly efficient, compressed, columnar storage format. This will minimize the memory footprint in Redis and speed up serialization/deserialization.
*   **Key Naming Convention:** `snapshot:{session_id}:{step_id}`

### 2.2. API Endpoints

The following endpoints will be added to the `databridge-researcher` API:

#### `GET /api/v1/viewer/snapshots/{session_id}`

*   **Purpose:** Lists all available data snapshots for a given session.
*   **Response Body:**
    ```json
    {
      "session_id": "session-xyz-123",
      "snapshots": [
        {
          "step_id": "step-abc-456",
          "tool_name": "load_csv",
          "timestamp": "2026-02-02T14:30:05Z",
          "data_shape": { "rows": 1500, "columns": 12 },
          "retrieval_url": "/api/v1/viewer/snapshot/session-xyz-123/step-abc-456"
        },
        ...
      ]
    }
    ```

#### `GET /api/v1/viewer/snapshot/{session_id}/{step_id}`

*   **Purpose:** Retrieves a specific data snapshot.
*   **Query Parameters:**
    *   `format` (optional): The desired output format. Can be `csv` or `json`. Defaults to `json`.
*   **Process:**
    1.  The API will fetch the Parquet-formatted data from Redis using the key `snapshot:{session_id}:{step_id}`.
    2.  It will deserialize the Parquet data into a DataFrame.
    3.  It will then serialize the DataFrame to the user's requested format (`csv` or `json`).
*   **Response:** The data in the requested format.

## 3. Architecture for Workflow Visualization

### 3.1. The "Workflow Trace"

To build the diagram, we need a detailed log of all actions.

*   **Mechanism:** I will extend the `AuditLogger` in the `databridge-core` library to create a `WorkflowTrace`.
*   **Trace Content:** For each tool execution, the trace will capture:
    *   `session_id`: The unique ID for the user's session.
    *   `step_id`: A unique ID for this specific execution step.
    *   `tool_name`: The name of the executed tool (e.g., `load_csv`).
    *   `status`: `success` or `failure`.
    *   `start_time` and `end_time`.
    *   `input_snapshots`: A list of `step_id`s that were used as input for this tool.
    *   `output_snapshot`: The `step_id` of the data generated by this tool.
    *   `data_shape`: The shape of the output data (rows, columns).

### 3.2. Diagram Generation

*   **New MCP Tool:** A new MCP tool named `export_workflow_diagram` will be created in the `databridge-librarian`.
*   **Process:**
    1.  The tool will fetch all `WorkflowTrace` entries for the current `session_id`.
    2.  It will construct a directed graph from the traces, with each tool execution as a node and the input/output relationships as edges.
    3.  It will use the **Graphviz** library to generate a visual representation of this graph in SVG format.
    4.  This SVG will be embedded into a self-contained HTML file. The HTML will include basic styling and metadata about the workflow.
    5.  For Markdown export, it will save the SVG to a file and embed it in the `.md` file.
*   **Visual Content:** Each node in the diagram will display the tool name, its status, and the shape of the output data.

This architecture provides a clear and scalable path to implementing your vision. I am ready to proceed with the first implementation step.
